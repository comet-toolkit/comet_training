{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_punpy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM40nBwQDY7evd+aKCRtcm/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2-final"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "outputs": [],
      "source": [
        "<a href=\"https://colab.research.google.com/github/pdevis/punpy/blob/master/esa_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "q--XLoo4Z325"
      },
      "outputs": [],
      "source": [
        "**Examples of punpy usage**"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "4pxZ7fqqZ6jS"
      },
      "outputs": [],
      "source": [
        "We explore a case where we are trying to calibrate some L0 data to L1 and have:\n",
        "\n",
        "- A measurement function that uses L0 data, gains, and a dark signal in 5 wavelength bands\n",
        "\n",
        "- Random uncertainties and systematic uncertainties on the L0 data;\n",
        "\n",
        "- Random and systematic uncertainties on the gains;\n",
        "\n",
        "- Random uncertainties on the dark signals"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "MzrTVRTqaNE3"
      },
      "outputs": [],
      "source": [
        "We first install and import our punpy package (and numpy and matplotlib), and define an example measurement function and example data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install punpy==0.25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import punpy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# your measurement function\n",
        "def calibrate(L0,gains,dark):\n",
        "   return (L0-dark)*gains\n",
        "\n",
        "# your data\n",
        "wavs = np.array([350,450,550,650,750])\n",
        "L0 = np.array([0.43,0.8,0.7,0.65,0.9])\n",
        "dark = np.array([0.05,0.03,0.04,0.05,0.06])\n",
        "gains = np.array([23,26,28,29,31])\n",
        "\n",
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "L0_us = np.ones(5)*0.03  # systematic uncertainty of 0.03\n",
        "                         # (common between bands)\n",
        "gains_ur = np.array([0.5,0.7,0.6,0.4,0.1])  # random uncertainty\n",
        "gains_us = np.array([0.1,0.2,0.1,0.4,0.3])  # systematic uncertainty\n",
        "# (different for each band but fully correlated)\n",
        "dark_ur = np.array([0.01,0.002,0.006,0.002,0.015])  # random uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "MWb_xvSoaa6Y"
      },
      "outputs": [],
      "source": [
        "We note that here we use a very simple analytical function as our measurement function, but this could be replaced by a complex processing chain, without any change to the way punpy is used.\n",
        "After defining the data, the resulting uncertainty budget can then be calculated with punpy using the MC methods as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,np.zeros(5)])\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "print(\"L1:    \",L1)\n",
        "print(\"L1_ur: \",L1_ur)\n",
        "print(\"L1_us: \",L1_us)\n",
        "print(\"L1_ut: \",L1_ut)\n",
        "print(\"L1_cov:\\n\",L1_cov)\n",
        "print(\"L1_corr:\\n\",L1_corr)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "h_td3OAXcUXT"
      },
      "outputs": [],
      "source": [
        "We then define some plots to inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_plots_L1(L1,L1_ur=None,L1_us=None,L1_ut=None,L1_corr=None):\n",
        "  if L1_cov is not None:\n",
        "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "  else:\n",
        "    fig,ax1 = plt.subplots(1,figsize=(5,5))\n",
        "\n",
        "  ax1.plot(wavs,L1,\"o\")\n",
        "  if L1_ur is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ur,label=\"random uncertainty\",capsize=5)\n",
        "  if L1_us is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_us,label=\"systematic uncertainty\",capsize=5)\n",
        "  if L1_ut is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ut,label=\"total uncertainty\",capsize=5)\n",
        "  ax1.legend()\n",
        "  ax1.set_xlabel(\"wavelength (nm)\")\n",
        "  ax1.set_ylabel(\"radiance\")\n",
        "  ax1.set_title(\"L1 uncertainties\")\n",
        "  if L1_cov is not None:\n",
        "    ax2.set_title(\"L1 correlation\")\n",
        "    cov_plot=ax2.imshow(L1_corr)\n",
        "    plt.colorbar(cov_plot,ax=ax2)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "vzSUig-Tei-T"
      },
      "outputs": [],
      "source": [
        "and make the plots for the L1 data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "**Correlated errors**"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "aGJvOAjLaqOb"
      },
      "outputs": [],
      "source": [
        "In addition to propagating random (uncorrelated) and systematic (fully correlated) uncertainties it is also possible to propagate uncertainties associated with structured errors. If we know the covariance matrix for each of the input quantities, it is straigtforward to propagate these. In the below example we assume the L0 data and dark data to be uncorrelated (their covariance matrix is a, diagonal matrix) and gains to be a custom covariance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "dark_ur = np.array([0.01,0.002,0.006,0.002,0.015])  # random uncertainty\n",
        "\n",
        "L0_cov=punpy.convert_corr_to_cov(np.eye(len(L0_ur)),L0_ur)\n",
        "dark_cov=punpy.convert_corr_to_cov(np.eye(len(dark_ur)),dark_ur )\n",
        "gains_cov= np.array([[0.45,0.35,0.30,0.20,0.05],\n",
        "                    [0.35,0.57,0.32,0.30,0.07],\n",
        "                    [0.30,0.32,0.56,0.24,0.06],\n",
        "                    [0.20,0.30,0.24,0.44,0.04],\n",
        "                    [0.05,0.07,0.06,0.04,0.21]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ut,L1_corr=prop.propagate_cov(calibrate,[L0,gains,dark],\n",
        "                                 [L0_cov,gains_cov,dark_cov],return_corr=True)\n",
        "\n",
        "make_plots_L1(L1,L1_ut=L1_ut,L1_corr=L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "In addition to having a correlation along one or more dimensions of a given variable, it is also possible two variables are correlated. This can be specified in punpy by using the corr_between keyword. In the example below, the systematic errors in the darks and L0 data are fully correlated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "\n",
        "corr_var=np.array([[1,0,1],[0,1,0],[1,0,1]])\n",
        "\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,L0_us],corr_between=corr_var)\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "The above results were generated using a Monte Carlo Method. The law of propagation of uncertainty can also be used to propagate the uncertainties. In this case, the Jacobian is used in the propagation. The Jacobian can be specified manually, or if not will be calculated through numerical differentiation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.LPUPropagation(parallel_cores=2)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "\n",
        "def J_calibrate(L0,gains,dark):\n",
        "    Jac_x1 = np.diag(gains)\n",
        "    Jac_x2 = np.diag(L0-dark)\n",
        "    Jac_x3 = np.diag(-gains)\n",
        "    Jac = np.concatenate((Jac_x1, Jac_x2, Jac_x3)).T\n",
        "    return Jac\n",
        "    \n",
        "Jx=J_calibrate(L0,gains,dark)\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur],Jx=Jx)\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,np.zeros(5)],Jx=Jx,corr_between=corr_var)\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "**punpy for data with more dimensions**"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {
        "id": "u9UXNYPKat5M"
      },
      "outputs": [],
      "source": [
        "In reality, propagation of uncertainty in EO is applied to larger datasets with higher dimensionality. Instead of the above 5 datapoints, we might have 5 wavelengths but 100 by 50 pixel images for each of these wavelengths. These can offcourse also be handled by punpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your data\n",
        "wavs = np.array([350,450,550,650,750])\n",
        "\n",
        "L0 = np.tile([0.43,0.8,0.7,0.65,0.9],(50,100,1)).T\n",
        "L0 = L0 + np.random.normal(0.0,0.05,L0.shape)\n",
        "\n",
        "dark = np.tile([0.05,0.03,0.04,0.05,0.06],(50,100,1)).T\n",
        "gains = np.tile([23,26,28,29,31],(50,100,1)).T\n",
        "\n",
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "L0_us = np.ones((5,100,50))*0.03  # systematic uncertainty of 0.03\n",
        "                         # (common between bands)\n",
        "\n",
        "gains_ur = np.tile(np.array([0.5,0.7,0.6,0.4,0.1]),(50,100,1)).T  # random uncertainty\n",
        "gains_us = np.tile(np.array([0.1,0.2,0.1,0.4,0.3]),(50,100,1)).T  # systematic uncertainty\n",
        "# (different for each band but fully correlated)\n",
        "dark_ur = np.tile(np.array([0.01,0.002,0.006,0.002,0.015]),(50,100,1)).T  # random uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(1000,)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur],repeat_dims=[1])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,None],repeat_dims=[1])\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "We then define a new function to plot images of the relative uncertainties in each band:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_plots_L1_image(wavs,L1,L1_u=None,c_range=[0,0.1]):\n",
        "  fig, axs = plt.subplots(1,len(wavs),figsize=(20,5))\n",
        "  \n",
        "  for i,ax in enumerate(axs):\n",
        "    ax.set_xlabel(\"x_pix\")\n",
        "    ax.set_ylabel(\"y_pix\")\n",
        "    ax.set_title(\"%s nm rel uncertainties\"%(wavs[i]))\n",
        "    im_plot=ax.imshow(L1_u[i]/L1[i],vmin=c_range[0],vmax=c_range[1])\n",
        "\n",
        "  plt.colorbar(im_plot)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "make_plots_L1_image(wavs,L1,L1_ur)\n",
        "make_plots_L1_image(wavs,L1,L1_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For multidimensional input quantities, it is often the case that a certain correlation structure is known along one of the dimensions, and that the other dimensions are either completely independent (random) or fully correlated (systematic). For example below, we know the correlation structure for the systematic uncertainties on the gains wrt wavelength, and consider each of the measurements to be fully correlted wrt the spatial dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gains_corr=np.array([[1.,0.14123392,0.12198785,0.07234254,0.01968095],\n",
        " [0.14123392,1.,0.1350783,0.12524757,0.0095603 ],\n",
        " [0.12198785,0.1350783,1.,0.1041107,0.02890266],\n",
        " [0.07234254,0.12524757,0.1041107,1.,0.01041678],\n",
        " [0.01968095,0.0095603,0.02890266,0.01041678,1.]])\n",
        "\n",
        "L1_us,L1_us_corr=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [None,gains_us,None],repeat_dims=[1,2],corr_x=[None,gains_corr,None],return_corr=True)\n",
        "\n",
        "make_plots_L1_image(wavs,L1,L1_us)\n",
        "make_plots_L1(np.mean(L1,axis=(1,2)),L1_us=np.mean(L1_us,axis=(1,2)),L1_corr=L1_us_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the returned correlation matrix is again wrt wavelength, and the correlation structure of the repeated measurements is the same as it was in the inputs. In the above example, the uncertainties on the L0 and darks are set to None, and are thus not included. However, it is possible to include these, even if they have a different correlation structure than the uncertainties on the gains. In the example below, we repeat the same, but now include systematic uncertainties on the L0, that are fully correlated. It can be seem in this case we can just set corr_x to None, in which case it will default to a full correlation (because we are using the propagate_systematic function). If we were using propagate_random, it would default to independent errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L1_us,L1_us_corr=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,None],repeat_dims=[1,2],corr_x=[None,gains_corr,None],return_corr=True)\n",
        "\n",
        "make_plots_L1_image(wavs,L1,L1_us)\n",
        "make_plots_L1(np.mean(L1,axis=(1,2)),L1_us=np.mean(L1_us,axis=(1,2)),L1_corr=L1_us_corr)"
      ]
    }
  ]
}