{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q--XLoo4Z325"
      },
      "source": [
        "**LPS hands-on training session: CoMet Toolkit: Uncertainties made easy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pxZ7fqqZ6jS"
      },
      "source": [
        "Within this training session, we will run through\n",
        "\n",
        "intro"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzrTVRTqaNE3"
      },
      "source": [
        "We first install the obsarray package (flag handling and accessing uncertainties), the punpy package (uncertainty propagation) and the matheo package (for band integration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7yYvW9g_YIF"
      },
      "outputs": [],
      "source": [
        "!pip install obsarray>=1.0.1\n",
        "!pip install punpy>=1.0.4\n",
        "!pip install matheo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrmiBNa_YIG"
      },
      "source": [
        "We also import all the relevant python packages we use in this training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qBuq1U4_YIG"
      },
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import punpy\n",
        "from matheo.band_integration import band_integration\n",
        "from obsarray.templater.dataset_util import DatasetUtil\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1IMxZ3i_YIG"
      },
      "source": [
        "If this import fails, it is likely because the pip installation has not properly updated in the Google colab session. Please restart session (in runtime tab above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCkeVKqO_YIH"
      },
      "source": [
        "Next, we clone the comet_training repository, which contains all the datasets used in this training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWAS_E1v_YIH"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/comet-toolkit/comet_training.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km3vk6bG_YIH"
      },
      "source": [
        "**Exercise 1: simple sensor calibration example**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KteypVd_YIH"
      },
      "source": [
        "Explain purpose of exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulwnPRAV_YIH"
      },
      "outputs": [],
      "source": [
        "# your measurement function\n",
        "def calibrate(L0,gains,dark):\n",
        "   return (L0-dark)*gains\n",
        "\n",
        "# your data\n",
        "wavs = np.array([350,450,550,650,750])\n",
        "L0 = np.array([0.43,0.8,0.7,0.65,0.9])\n",
        "dark = np.array([0.05,0.03,0.04,0.05,0.06])\n",
        "gains = np.array([23,26,28,29,31])\n",
        "\n",
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "L0_us = np.ones(5)*0.03  # systematic uncertainty of 0.03\n",
        "                         # (common between bands)\n",
        "gains_ur = np.array([0.5,0.7,0.6,0.4,0.1])  # random uncertainty\n",
        "gains_us = np.array([0.1,0.2,0.1,0.4,0.3])  # systematic uncertainty\n",
        "# (different for each band but fully correlated)\n",
        "dark_ur = np.array([0.01,0.002,0.006,0.002,0.015])  # random uncertainty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rg4Kqo5_YII"
      },
      "source": [
        "We note that here we use a very simple analytical function as our measurement function, but this could be replaced by a complex processing chain, without any change to the way punpy is used.\n",
        "After defining the data, the resulting uncertainty budget can then be calculated with punpy using the MC methods as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjryZYQf_YII"
      },
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,np.zeros(5)])\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "print(\"L1:    \",L1)\n",
        "print(\"L1_ur: \",L1_ur)\n",
        "print(\"L1_us: \",L1_us)\n",
        "print(\"L1_ut: \",L1_ut)\n",
        "print(\"L1_cov:\\n\",L1_cov)\n",
        "print(\"L1_corr:\\n\",L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFDJu6bU_YII"
      },
      "source": [
        "We then define some plots to inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GK-IEvnA_YII"
      },
      "outputs": [],
      "source": [
        "def make_plots_L1(L1,L1_ur=None,L1_us=None,L1_ut=None,L1_corr=None):\n",
        "  if L1_cov is not None:\n",
        "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "  else:\n",
        "    fig,ax1 = plt.subplots(1,figsize=(5,5))\n",
        "\n",
        "  ax1.plot(wavs,L1,\"o\")\n",
        "  if L1_ur is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ur,label=\"random uncertainty\",capsize=5)\n",
        "  if L1_us is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_us,label=\"systematic uncertainty\",capsize=5)\n",
        "  if L1_ut is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ut,label=\"total uncertainty\",capsize=5)\n",
        "  ax1.legend()\n",
        "  ax1.set_xlabel(\"wavelength (nm)\")\n",
        "  ax1.set_ylabel(\"radiance\")\n",
        "  ax1.set_title(\"L1 uncertainties\")\n",
        "  if L1_cov is not None:\n",
        "    ax2.set_title(\"L1 correlation\")\n",
        "    cov_plot=ax2.imshow(L1_corr)\n",
        "    plt.colorbar(cov_plot,ax=ax2)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weg3_FUW_YIJ"
      },
      "source": [
        "and make the plots for the L1 data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmMlAu4H_YIJ"
      },
      "outputs": [],
      "source": [
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4ghLOyo_YIJ"
      },
      "source": [
        "*Correlated errors*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLKoGxWk_YIJ"
      },
      "source": [
        "In addition to propagating random (uncorrelated) and systematic (fully correlated) uncertainties it is also possible to propagate uncertainties associated with structured errors. If we know the covariance matrix for each of the input quantities, it is straigtforward to propagate these. In the below example we assume the L0 data and dark data to be uncorrelated (their covariance matrix is a, diagonal matrix) and gains to be a custom covariance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jw1T8nBE_YIJ"
      },
      "outputs": [],
      "source": [
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "dark_ur = np.array([0.01,0.002,0.006,0.002,0.015])  # random uncertainty\n",
        "\n",
        "L0_cov=punpy.convert_corr_to_cov(np.eye(len(L0_ur)),L0_ur)\n",
        "dark_cov=punpy.convert_corr_to_cov(np.eye(len(dark_ur)),dark_ur )\n",
        "gains_cov= np.array([[0.45,0.35,0.30,0.20,0.05],\n",
        "                    [0.35,0.57,0.32,0.30,0.07],\n",
        "                    [0.30,0.32,0.56,0.24,0.06],\n",
        "                    [0.20,0.30,0.24,0.44,0.04],\n",
        "                    [0.05,0.07,0.06,0.04,0.21]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTPqc4gD_YIJ"
      },
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ut,L1_corr=prop.propagate_cov(calibrate,[L0,gains,dark],\n",
        "                                 [L0_cov,gains_cov,dark_cov],return_corr=True)\n",
        "\n",
        "make_plots_L1(L1,L1_ut=L1_ut,L1_corr=L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8x3Rmug_YIK"
      },
      "source": [
        "In addition to having a correlation along one or more dimensions of a given variable, it is also possible two variables are correlated. This can be specified in punpy by using the corr_between keyword. In the example below, the systematic errors in the darks and L0 data are fully correlated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW5zxFmp_YIK"
      },
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "\n",
        "corr_var=np.array([[1,0,1],[0,1,0],[1,0,1]])\n",
        "\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,L0_us],corr_between=corr_var)\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnaYHTce_YIK"
      },
      "source": [
        "**punpy for data with more dimensions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1Y9RAz9_YIK"
      },
      "source": [
        "In reality, propagation of uncertainty in EO is applied to larger datasets with higher dimensionality. Instead of the above 5 datapoints, we might have 5 wavelengths but 100 by 50 pixel images for each of these wavelengths. These can offcourse also be handled by punpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2g0Z7n__YIK"
      },
      "outputs": [],
      "source": [
        "# your data\n",
        "wavs = np.array([350,450,550,650,750])\n",
        "\n",
        "L0 = np.tile([0.43,0.8,0.7,0.65,0.9],(50,100,1)).T\n",
        "L0 = L0 + np.random.normal(0.0,0.05,L0.shape)\n",
        "\n",
        "dark = np.tile([0.05,0.03,0.04,0.05,0.06],(50,100,1)).T\n",
        "gains = np.tile([23,26,28,29,31],(50,100,1)).T\n",
        "\n",
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "L0_us = np.ones((5,100,50))*0.03  # systematic uncertainty of 0.03\n",
        "                         # (common between bands)\n",
        "\n",
        "gains_ur = np.tile(np.array([0.5,0.7,0.6,0.4,0.1]),(50,100,1)).T  # random uncertainty\n",
        "gains_us = np.tile(np.array([0.1,0.2,0.1,0.4,0.3]),(50,100,1)).T  # systematic uncertainty\n",
        "# (different for each band but fully correlated)\n",
        "dark_ur = np.tile(np.array([0.01,0.002,0.006,0.002,0.015]),(50,100,1)).T  # random uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9p_YfT1m_YIK"
      },
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(1000,)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur],repeat_dims=[1])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,None],repeat_dims=[1])\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn-mRX2W_YIL"
      },
      "source": [
        "We then define a new function to plot images of the relative uncertainties in each band:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Xh5S3EX_YIL"
      },
      "outputs": [],
      "source": [
        "def make_plots_L1_image(wavs,L1,L1_u=None,c_range=[0,0.1]):\n",
        "  fig, axs = plt.subplots(1,len(wavs),figsize=(20,5))\n",
        "\n",
        "  for i,ax in enumerate(axs):\n",
        "    ax.set_xlabel(\"x_pix\")\n",
        "    ax.set_ylabel(\"y_pix\")\n",
        "    ax.set_title(\"%s nm rel uncertainties\"%(wavs[i]))\n",
        "    im_plot=ax.imshow(L1_u[i]/L1[i],vmin=c_range[0],vmax=c_range[1])\n",
        "\n",
        "  plt.colorbar(im_plot)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqEHjTau_YIL"
      },
      "outputs": [],
      "source": [
        "make_plots_L1_image(wavs,L1,L1_ur)\n",
        "make_plots_L1_image(wavs,L1,L1_us)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt5uZCZX_YIL"
      },
      "source": [
        "For multidimensional input quantities, it is often the case that a certain correlation structure is known along one of the dimensions, and that the other dimensions are either completely independent (random) or fully correlated (systematic). For example below, we know the correlation structure for the systematic uncertainties on the gains wrt wavelength, and consider each of the measurements to be fully correlted wrt the spatial dimensions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgUTnNWb_YIL"
      },
      "outputs": [],
      "source": [
        "gains_corr=np.array([[1.,0.14123392,0.12198785,0.07234254,0.01968095],\n",
        " [0.14123392,1.,0.1350783,0.12524757,0.0095603 ],\n",
        " [0.12198785,0.1350783,1.,0.1041107,0.02890266],\n",
        " [0.07234254,0.12524757,0.1041107,1.,0.01041678],\n",
        " [0.01968095,0.0095603,0.02890266,0.01041678,1.]])\n",
        "\n",
        "L1_us,L1_us_corr=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [None,gains_us,None],repeat_dims=[1,2],corr_x=[None,gains_corr,None],return_corr=True)\n",
        "\n",
        "make_plots_L1_image(wavs,L1,L1_us)\n",
        "make_plots_L1(np.mean(L1,axis=(1,2)),L1_us=np.mean(L1_us,axis=(1,2)),L1_corr=L1_us_corr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKkUIZ8p_YIM"
      },
      "source": [
        "summary statement"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "vhroda_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}