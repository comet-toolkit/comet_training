{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS%20Training%20-%20Exercise%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LPS Hands-on Training Session - CoMet Toolkit: Uncertainties made easy**\n",
        "\n",
        "#Exercise 2: Multi-Dimension Datasets\n",
        "\n",
        "## Objectives\n",
        "\n",
        "In this exercise we will cover:\n",
        "\n",
        "* How to use [**obsarray**](https://obsarray.readthedocs.io/en/latest/) to store error-correlation information for multi-dimensional measurement datasets - such as from Earth Observation.\n",
        "* Propagating uncertainties from these datasets through measurement functions using [**punpy**](https://punpy.readthedocs.io/en/latest/)."
      ],
      "metadata": {
        "id": "5-upOzlp_cLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Step 1* - Environment Setup\n",
        "\n",
        "As in Exercise 1, we start by collecting test data and installing and importing the required CoMet Toolkit packages --- [**obsarray**](https://obsarray.readthedocs.io/en/latest/) and [**punpy**](https://punpy.readthedocs.io/en/latest/)."
      ],
      "metadata": {
        "id": "geTn1KRyTOy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7yYvW9g_YIF"
      },
      "outputs": [],
      "source": [
        "# Clone the CoMet Training repository to access training data\n",
        "!git clone https://github.com/comet-toolkit/comet_training.git\n",
        "\n",
        "# Install & import CoMet Toolkit packages\n",
        "!pip install obsarray>=1.0.1\n",
        "!pip install punpy>=1.0.4\n",
        "\n",
        "import obsarray\n",
        "import punpy\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoMet Toolkit's [**obsarray**](https://obsarray.readthedocs.io/en/latest/) package is an extension to the popular [xarray](https://docs.xarray.dev/en/stable/) package.\n",
        "\n",
        "[`xarray.Dataset`](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#dataset)'s are objects in python that represent the data model of the [netCDF file format](https://www.unidata.ucar.edu/software/netcdf/). **obsarray** allows you to assign uncertainties to variables in xarray Datasets, with their associated error-correlation.\n",
        "\n",
        "This is achieved by using the CoMet Toolkit's draft [UNC Specification](https://comet-toolkit.github.io/unc_website/) metadata standard for dataset variable attributes. So such objects are portable, and can be stored to and from netCDF files on disc.\n"
      ],
      "metadata": {
        "id": "48g5MLJXIJsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Step 2* - Interfacing with a Measurement Dataset using **obsarray**\n",
        "\n",
        "In this step of the exercise, we will explore how to define and interact with the *uncertainty variables* (i.e., uncertainty components) associated with *observation variables* in measurement datasets using **obsarray**.\n",
        "\n",
        "Our example will be a multi-spectral dataset of Level 1 (L1) [Brightness Temperatures](https://en.wikipedia.org/wiki/Brightness_temperature) (BT) from the AVHRR sensor on MetOp-A. This [dataset](https://catalogue.ceda.ac.uk/uuid/14a8098d70714cc1bf38f9dbcb82e5ed/) was created as part of the [FIDUCEO](https://research.reading.ac.uk/fiduceo/) project.\n",
        "\n",
        "Here we open an extract contained in a [netCDF](https://www.unidata.ucar.edu/software/netcdf/) file, which has an observation variable -- `brightness_temperature` -- with the following dimensions:\n",
        "\n",
        "* $x$, along track -- 400 pixels at 1 km resolution\n",
        "* $y$, across track -- 400 pixels at 1 km resolution\n",
        "* $band$, spectral bands -- 2 bands centred on $\\sim$11 μm and 12 μm\n"
      ],
      "metadata": {
        "id": "S1VYgCHhTSm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open xarray.Dataset from netCDF file\n",
        "dataset_path = \"avhrr_dataset.nc\"\n",
        "avhrr_ds = xr.open_dataset(dataset_path)\n",
        "\n",
        "# inspect dataset\n",
        "print(avhrr_ds)\n",
        "avhrr_ds[\"brightness_temperature\"][0].plot()"
      ],
      "metadata": {
        "id": "Wimlaz9hITFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After import, **obsarray** functionality is accessed throught the `unc` \"[accessor](https://docs.xarray.dev/en/stable/internals/extending-xarray.html)\" -- which looks like a new method that becomes available on xarray Datasets.\n",
        "\n",
        "We can use this to [assign an *uncertainty variable*](https://obsarray.readthedocs.io/en/latest/content/user/unc_accessor.html#adding-removing-uncertainty-components) to the `brightness_temperature`, in a very similar way to adding a normal variable to an xarray Dataset:"
      ],
      "metadata": {
        "id": "kmIJ9Yp7R_aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define u_noise values - set as 1%\n",
        "u_cal_values = avhrr_ds[\"brightness_temperature\"].values * 0.01\n",
        "\n",
        "# add an uncertainty component associated with noise error to the brightness temperature\n",
        "avhrr_ds.unc[\"brightness_temperature\"][\"u_noise\"] = ([\"x\", \"y\", \"band\"], u_noise_values)\n",
        "\n",
        "print(\"Dataset Variables: \" + str(avhrr_ds.keys())"
      ],
      "metadata": {
        "id": "zqXPilMwSgpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncertainty variables have an assocaited error-correlation structure -- since we didn't define this for `u_noise`, it is assumed to be random (i.e., errors are uncorrelated between pixels).\n",
        "\n",
        "Next let's add a calibration uncertainty component, `u_cal`, with a more complicated error-correlation structure using the `err_corr` attribute. This uses the [error-correlation parameterisations](https://comet-toolkit.github.io/unc_website/specification/draft-v0.1/unc_specification.html#appendix-a-error-correlation-parameterisations) defined by the draft UNC Specfication (it is also possible to add custom error-correlation parameterisations).\n",
        "\n",
        "Let's set the pixel errors associated with `u_cal` to be systematic (i.e., the same/common) in the `x` and `y` dimension and defined by a custom matrix in the `band` dimension."
      ],
      "metadata": {
        "id": "tsWrx8i9W235"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create cross-channel error-correlation matrix\n",
        "chl_err_corr_matrix = np.array([[1.0, 0.7],[0.7, 1.0]])\n",
        "avhrr_ds[\"chl_err_corr_matrix\"] = ((\"band1\", \"band2\"), chl_err_corr_matrix)\n",
        "\n",
        "# use this to define error-correlation parameterisation attribute\n",
        "err_corr_def = [\n",
        "    # fully systematic in the x and y dimension\n",
        "    {\n",
        "        \"dim\": [\"x\", \"y\"],\n",
        "        \"form\": \"systematic\",\n",
        "        \"params\": [],\n",
        "        \"units\": []\n",
        "    },\n",
        "    # defined by err-corr matrix var in band dimension\n",
        "    {\n",
        "        \"dim\": [\"band\"],\n",
        "        \"form\": \"err_corr_matrix\",\n",
        "        \"params\": [\"chl_err_corr_matrix\"],  # defines link to err-corr matrix var\n",
        "        \"units\": []\n",
        "    }\n",
        "]\n",
        "\n",
        "# define u_cal values - set as 5%\n",
        "u_cal_values = avhrr_ds[\"brightness_temperature\"].values * 0.05\n",
        "\n",
        "# add an uncertainty component associated with calibration error to the brightness temperature\n",
        "avhrr_ds.unc[\"brightness_temperature\"][\"u_cal\"] = ([\"x\", \"y\", \"band\"], u_cal_values, {\"err_corr\": err_corr_def})"
      ],
      "metadata": {
        "id": "MsbjwLUMWqcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now interface with this information"
      ],
      "metadata": {
        "id": "gkwaWcdAc3Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4StS115iBhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Step 3* - Uncertainty Propagation\n",
        "\n",
        "Thermal Infrared multi-spectral data, like our example AVHRR dataset, is used to develop Level 2 (L2) Climate Data Records (CDRs) such as Sea or Land Surface Temperature (SST or LST). SST/LST retriavals account for the atmosphere to evaluate the surface temperature from the top of atmosphere L1 brightness temperature.\n",
        "\n",
        "A widely approach for this is called the \"split window\" method. A simplified form of this algorithm could be represented as,\n",
        "\n",
        "$SST = a T_{11} - b T_{12}$\n",
        "\n",
        "where:\n",
        "\n",
        "* $T_{11}$​ is the brightness temperature in the 11 μm band\n",
        "* $T_{12}$​ is the brightness temperature in the 12 μm band\n",
        "* $a$ & $b$ are empirically derived retrieval coefficients\n",
        "\n",
        "For the purpose of the exercise, set $a=2$ and $b=1$.\n",
        "\n",
        "## **Exercise**\n",
        "\n",
        "Using what we learned in [Exercise 1](https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS_training_exercise1.ipynb), create a measurement function to apply the SST retrieval to our AVHRR dataset and propagate the uncertainties using **punpy**."
      ],
      "metadata": {
        "id": "hk0BoZECiCPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here"
      ],
      "metadata": {
        "id": "2LXATlPNl50l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Extension* - Propagating Dataset Uncertainties with the `MeasurementFunction` Class\n",
        "\n",
        "**punpy**'s [`MeasurementFunction`](https://punpy.readthedocs.io/en/latest/content/punpy_digital_effects_table.html#measurement-function) class enables a much simpler method for propagating the uncertainties of measurement datasets defined using **obsarray**. It is an alternative interface to the **punpy** propagation functions we used in Step 3.\n",
        "\n",
        "For this approach instead defining the measurement function as a python function, we define a measurement function class which should be a subclass of the punpy `MeasurementFunction` class. We can then use the method [`propagate_ds`](https://punpy.readthedocs.io/en/latest/content/punpy_digital_effects_table.html#functions-for-propagating-uncertainties) to propagate all dataset uncertainties in one go!"
      ],
      "metadata": {
        "id": "WXtbn00KmTsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitWindowSST(punpy.MeasurementFunction):\n",
        "  #\n",
        "  # define primary method of class - the measurement function\n",
        "  #\n",
        "  def meas_function(self, BT: np.ndarray) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Returns SST from input L1 BTs using split window method\n",
        "\n",
        "  :param BT: brightness temperature datacube\n",
        "  :returns: evaluated SST\n",
        "  \"\"\"\n",
        "\n",
        "  # set parameter values\n",
        "  a = 2\n",
        "  b = 1\n",
        "\n",
        "  # evaluate SST\n",
        "  sst = a * BT[0,:,:] - b * BT[1,:,:]\n",
        "\n",
        "  return sst\n",
        "\n",
        "  #\n",
        "  # define helper methods to configure class\n",
        "  #\n",
        "  def get_measurand_name_and_unit(self) -> tuple(str, str):\n",
        "    \"\"\"\n",
        "    For dataset evaluate by measurement function, returns a tuple of\n",
        "    measurand variable name and units\n",
        "\n",
        "    :returns: measurand name, measurand unit name\n",
        "    \"\"\"\n",
        "    return \"sst\", \"K\"\n",
        "\n",
        "  def get_argument_names(self) -> list[str]:\n",
        "    \"\"\"\n",
        "    Returns orders list input dataset variables names associated with\n",
        "    meas_function arguments\n",
        "\n",
        "    :returns: input dataset variable names\n",
        "    \"\"\"\n",
        "    return [\"brightness_temperature\"]\n",
        "\n",
        "\n",
        "# create punpy propagation object\n",
        "prop = MCPropagation(1000, dtype=\"float32\", verbose=False, parallel_cores=4)\n",
        "\n",
        "# Instatiate measurement function object with prop\n",
        "sst_ret = SplitWindowSST(prop=prop)\n",
        "\n",
        "# run uncertainty propagatoin\n",
        "sst_ds = sst_ret.propagate_ds(avhrr_ds)"
      ],
      "metadata": {
        "id": "IVUlxJvJmkm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**\n",
        "\n",
        "Adapt the `MeasurementFunction` class approach above to include error-covariance for the set of parameters $a$ and $b$."
      ],
      "metadata": {
        "id": "Zejvkr4WpcCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here"
      ],
      "metadata": {
        "id": "a5Ueqor2ptB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}