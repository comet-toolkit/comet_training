{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS%20Training%20-%20Exercise%202.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LPS hands-on training session: CoMet Toolkit: Uncertainties made easy**\n",
        "\n",
        "#Exercise 2: Multi-Dimension Datasets\n",
        "\n",
        "## Objectives\n",
        "\n",
        "In this exercise we will cover:\n",
        "\n",
        "* How to use [obsarray](https://obsarray.readthedocs.io/en/latest/) to store error-correlation information for multi-dimensional measurement datasets - such as from Earth Observation.\n",
        "* Propagating uncertainties from these datasets through measurement functions using [punpy](https://punpy.readthedocs.io/en/latest/)."
      ],
      "metadata": {
        "id": "5-upOzlp_cLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 - Environment Setup\n",
        "\n",
        "As in Exercise 1, we start by install and importing the required CoMet Toolkit packages --- [obsarray](https://obsarray.readthedocs.io/en/latest/) and [punpy](https://punpy.readthedocs.io/en/latest/)."
      ],
      "metadata": {
        "id": "geTn1KRyTOy6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7yYvW9g_YIF"
      },
      "outputs": [],
      "source": [
        "# Install & import CoMet Toolkit packages\n",
        "!pip install obsarray>=1.0.1\n",
        "!pip install punpy>=1.0.4\n",
        "\n",
        "import obsarray\n",
        "import punpy\n",
        "import xarray as xr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CoMet Toolkit's [**obsarray**](https://obsarray.readthedocs.io/en/latest/) package is an extension to the popular [xarray](https://docs.xarray.dev/en/stable/) package.\n",
        "\n",
        "[`xarray.Dataset`](https://docs.xarray.dev/en/stable/user-guide/data-structures.html#dataset)'s are objects in python that represent of the data model from the [netCDF file format](https://www.unidata.ucar.edu/software/netcdf/). **obsarray** allows you to assign uncertainties to variables in xarray Datasets, with their associated error-correlation.\n",
        "\n",
        "This is achieved by using the CoMet Toolkit's [UNC Specification](https://comet-toolkit.github.io/unc_website/) metadata standard for uncertainty. So such objects are portable, and can be stored to and from disc.\n"
      ],
      "metadata": {
        "id": "48g5MLJXIJsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 - Interacting with a Measurement Dataset using obsarray\n",
        "\n",
        "In this step of the exercise, we will explore how to define and interact with the *uncertainty variables* (i.e., uncertainty components) associated with *observation variables* in measurement datasets using obsarray.\n",
        "\n",
        "Our example will be a multi-spectral dataset of Level 1 [Brightness Temperatures](https://en.wikipedia.org/wiki/Brightness_temperature) from the AVHRR sensor on MetOp-A. This [dataset](https://catalogue.ceda.ac.uk/uuid/14a8098d70714cc1bf38f9dbcb82e5ed/) was created as part of the [FIDUCEO](https://research.reading.ac.uk/fiduceo/) project.\n",
        "\n",
        "Here we open an extract from [netCDF](https://www.unidata.ucar.edu/software/netcdf/), which has an observation variable with the following dimensions:\n",
        "\n",
        "* $x$, along track -- 400 pixels at 1 km resolution\n",
        "* $y$, across track -- 400 pixels at 1 km resolution\n",
        "* $band$, spectral bands -- 2 bands centred on $\\sim$11 and 12 $\\mu$m\n"
      ],
      "metadata": {
        "id": "S1VYgCHhTSm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open xarray.Dataset from netCDF file\n",
        "dataset_path = \"avhrr_dataset.nc\"\n",
        "avhrr_ds = xr.open_dataset(dataset_path)\n",
        "\n",
        "# inspect dataset\n",
        "print(avhrr_ds)\n",
        "avhrr_ds[\"brightness_temperature\"][0].plot()"
      ],
      "metadata": {
        "id": "Wimlaz9hITFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After import, **obsarray** functionality is accessed throught the `unc` \"[accessor](https://docs.xarray.dev/en/stable/internals/extending-xarray.html)\" -- which looks like a new [link text](https://)method that becomes available on xarray Datasets.\n",
        "\n",
        "We can use this to [assign an *uncertainty variable*](https://obsarray.readthedocs.io/en/latest/content/user/unc_accessor.html#adding-removing-uncertainty-components) to the `brightness_temperature`, in a very similar way to adding a normal variable to an xarray Dataset:"
      ],
      "metadata": {
        "id": "kmIJ9Yp7R_aR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add an uncertainty component associated with noise error to the brightness temperature\n",
        "avhrr_ds.unc[\"brightness_temperature\"][\"u_noise\"] = ()\n",
        "\n",
        "print(\"Dataset Variables: \" + str(avhrr_ds.keys())"
      ],
      "metadata": {
        "id": "zqXPilMwSgpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncertainty variables have an assocaited error-correlation structure -- since we didn't define this for `u_noise` it is assumed to be random (i.e., errors are uncorrelated between pixels.\n",
        "\n",
        "Next let's add a calibration uncertainty component, `u_cal`, with a more complicated error-correlation structure using the `err_corr` attribute. This uses the [error-correlation parameterisations](https://comet-toolkit.github.io/unc_website/specification/draft-v0.1/unc_specification.html#appendix-a-error-correlation-parameterisations) defined by the UNC Specfication.\n",
        "\n",
        "Let's set errors associated with `u_cal` to be systematic (i.e., the same/common) in the `x` and `y` dimension and defined by a custom matrix in the `band` dimension."
      ],
      "metadata": {
        "id": "tsWrx8i9W235"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create cross-channel error-correlation matrix\n",
        "chl_err_corr_matrix = np.array([[1.0, 0.7],[0.7, 1.0]])\n",
        "avhrr_ds[\"chl_err_corr_matrix\"] = ((\"band1\", \"band2\"), chl_err_corr_matrix)\n",
        "\n",
        "# use this to define error-correlation parameterisation attribute\n",
        "err_corr_def = [\n",
        "    # fully systematic in the x and y dimension\n",
        "    {\n",
        "        \"dim\": [\"x\", \"y\"],\n",
        "        \"form\": \"systematic\",\n",
        "        \"params\": [],\n",
        "        \"units\": []\n",
        "    },\n",
        "    # defined by err-corr matrix var in band dimension\n",
        "    {\n",
        "        \"dim\": [\"band\"],\n",
        "        \"form\": \"err_corr_matrix\",\n",
        "        \"params\": [\"chl_err_corr_matrix\"],  # defines link to err-corr matrix var\n",
        "        \"units\": []\n",
        "    }\n",
        "]\n",
        "\n",
        "# define u_cal values - set as 5%\n",
        "u_cal_values = avhrr_ds[\"brightness_temperature\"].values * 0.05\n",
        "\n",
        "# add an uncertainty component associated with calibration error to the brightness temperature\n",
        "avhrr_ds.unc[\"brightness_temperature\"][\"u_cal\"] = ([\"x\", \"y\", \"band\"], u_cal_values, {\"err_corr\": err_corr_def})"
      ],
      "metadata": {
        "id": "MsbjwLUMWqcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UcQk_paUd0vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now interface with this information"
      ],
      "metadata": {
        "id": "gkwaWcdAc3Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T4StS115iBhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - Uncertainty Propagation\n",
        "\n",
        "Thermal Infrared multi-spectral data, like our example AVHRR dataset, is commonly used to evaluate sea or land surface temperature (SST or LST). SST/LST retriavals account for the atmosphere to evaluate the surface temperature from the top of atmosphere brightness temperature.\n",
        "\n",
        "A widely approach for this is called the \"split window\" method. A simplified form of this algorithm can be defined as,\n",
        "\n",
        "$SST = a T_{11} + b T_{12}$\n",
        "\n",
        "where:\n",
        "* $T_{11}$​ is the brightness temperature in the 11 μm channel\n",
        "* $T_{12}$​ is the brightness temperature in the 12 μm channel\n",
        "* $a$ & $b$ are empirically derived retrieval coefficients\n",
        "\n",
        "##Exercise\n",
        "\n",
        "Using what we learned in [Exercise 1](https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS_training_exercise1.ipynb), create a measurement function to apply the SST retrieval to our AVHRR dataset and propagate the uncertainties using punpy."
      ],
      "metadata": {
        "id": "hk0BoZECiCPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here"
      ],
      "metadata": {
        "id": "2LXATlPNl50l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# *Extension* - Propagating Dataset Uncertainties with the `MeasurementFunction` Class\n",
        "\n",
        "**punpy**'s [`MeasurementFunction`](https://punpy.readthedocs.io/en/latest/content/punpy_digital_effects_table.html#measurement-function) class enables a much simpler method for propagating the uncertainties of measurement datasets defined using **obsarray**. It is an alternative interface to the punpy propagation functions we used in Step 3.\n",
        "\n",
        "For this approach instead defining the measurement function as a python function, we define a measurement function class which should be a subclass of the punpy `MeasurementFunction` class. We can then use the method `propagate_ds` to propagate all dataset uncertainties in one go!"
      ],
      "metadata": {
        "id": "WXtbn00KmTsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitWindowSST(punpy.MeasurementFunction):\n",
        "  def meas_function(self, BT):\n",
        "  \"\"\"\n",
        "  Split window method retrieval measurement function\n",
        "  \"\"\"\n",
        "\n",
        "  return a * BT[0,:,:] + b * BT[1,:,:]\n",
        "\n",
        "  # define variable names for output dataset\n",
        "  def get_measurand_name_and_unit(self):\n",
        "    return \"sst\", \"K\"\n",
        "\n",
        "  # associate input dataset names with meas_function arguments\n",
        "  def get_argument_names(self):\n",
        "    return [\"brightness_temperature\"]\n",
        "\n",
        "\n",
        "# create punpy propagation object\n",
        "prop = MCPropagation(1000, dtype=\"float32\", verbose=False, parallel_cores=4)\n",
        "\n",
        "# instatiate measurement function object with prop\n",
        "gl = IdealGasLaw(prop=prop)\n",
        "\n",
        "# run uncertainty propagatoin\n",
        "ds_y = gl.propagate_ds(ds_pres, ds_nmol, ds_temp)"
      ],
      "metadata": {
        "id": "IVUlxJvJmkm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "Adapt the `MeasurementFunction` class approach above to include uncertainty on the parameters $a$ and $b$."
      ],
      "metadata": {
        "id": "Zejvkr4WpcCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enter your code here"
      ],
      "metadata": {
        "id": "a5Ueqor2ptB2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}