{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS_training_exercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q--XLoo4Z325"
      },
      "source": [
        "**LPS Hands-on Training Session - CoMet Toolkit: Uncertainties made easy**\n",
        "\n",
        "#Exercise 1: Explore some of the basic functionality of the punpy tool.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "In this exercise we will cover:\n",
        "\n",
        "* Get familiar with the [**punpy**](https://punpy.readthedocs.io/en/latest/) tool.\n",
        "* Propagating uncertainties on manually provided input data through a simple measurement functions using [**punpy**](https://punpy.readthedocs.io/en/latest/).\n",
        "* Explore the various ways uncertainties with different error correlations can be propagated.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MzrTVRTqaNE3"
      },
      "source": [
        "## *Step 1* - Environment Setup\n",
        "\n",
        "We first install the obsarray package (flag handling and accessing uncertainties), the punpy package (uncertainty propagation) and the matheo package (for band integration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install obsarray>=1.0.1\n",
        "!pip install punpy>=1.0.4"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then import the relevant python packages we use in this training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import punpy"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If this import fails, it is likely because the pip installation has not properly updated in the Google colab session. Please restart session (in runtime tab above)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Step 2* - define measurement function and input data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this exercise, the aim is to get familiar with the basic functionality of punpy. Here, punpy will be used as a standalone tool (i.e. without combining it with obsarray functionality). We will use an example of a very basic sensor calibration, where we have some digital numbers for the signal (referred to as L0) and the gains (typically obtained from a lab calibration) to convert these to a physical quantity (referred to as L1). This could e.g. be for a radiance measurement by an insitu instrument.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we define our measurement function. For use in punpy, this measurement function needs to be written as a Python function that takes the input quantities (on which we have uncertainties available) as arguments and the measurand (to which we want to propagate the uncertainties) as return. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your measurement function\n",
        "def calibrate(L0,gains):\n",
        "   return L0*gains"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the measurement function is a very simple analytical function. However in practise, this measurement function can contain as much complexity (including calls to other packages/external software, ...) as needed. The measurement function is to some extend treated as a black box, as long as the input quantities and measurand are structured as expected."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define some example input data. For your own usecase, you would need to have this information available from other sources (i.e. the uncertainties on your inputs needs to be understood prior to using punpy). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your data\n",
        "wavs = np.array([350,450,550,650,750])\n",
        "L0 = np.array([0.43,0.8,0.7,0.65,0.9])\n",
        "gains = np.array([23,26,28,29,31])\n",
        "\n",
        "# your uncertainties\n",
        "L0_ur = L0*0.05                             # 5% random uncertainty\n",
        "L0_us = np.ones(5)*0.03                     # systematic uncertainty of 0.03\n",
        "                                            # (common between bands)\n",
        "gains_ur = np.array([0.5,0.7,0.6,0.4,0.1])  # random uncertainty\n",
        "gains_us = np.array([0.1,0.2,0.1,0.4,0.3])  # systematic uncertainty\n",
        "                                            # (different for each band but fully correlated)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Step 3* - Propagate the random and systematic uncertainties separately \n",
        "\n",
        "After defining the data, the resulting uncertainty budget can then be calculated with punpy using the MC method. First, we separately propagate the random and systematic uncertainties, and then combine the resulting L1 uncertainties. The error correlations are also combined using some helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialise a punpy MCpropagation object with 10000 MC samples\n",
        "prop=punpy.MCPropagation(10000)     \n",
        "\n",
        "# apply the measuremnet function to calculate the measurand from the input quantities\n",
        "L1=calibrate(L0,gains)\n",
        "\n",
        "#propagate random uncertainties\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains],\n",
        "      [L0_ur,gains_ur])\n",
        "\n",
        "#propagate systematic uncertainties\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains],\n",
        "      [L0_us,gains_us])\n",
        "\n",
        "#combine random and systematic uncertainties \n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "\n",
        "#calculate random and systematic error correlation matrices (this is done by first combining covariances)\n",
        "L1_cov=(punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)                   # random uncertainties have an identity matrix as the error correlation \n",
        "        + punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us))  # systematic uncertainties have a matrix full of ones as the error correlation\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "\n",
        "#print the results\n",
        "print(\"L1:    \",L1)\n",
        "print(\"L1_ur: \",L1_ur)\n",
        "print(\"L1_us: \",L1_us)\n",
        "print(\"L1_ut: \",L1_ut)\n",
        "print(\"L1_cov:\\n\",L1_cov)\n",
        "print(\"L1_corr:\\n\",L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define some plots to inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#define plot to show results \n",
        "def make_plots_L1(L1,L1_ur=None,L1_us=None,L1_ut=None,L1_corr=None):\n",
        "  if L1_cov is not None:\n",
        "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "  else:\n",
        "    fig,ax1 = plt.subplots(1,figsize=(5,5))\n",
        "\n",
        "  ax1.plot(wavs,L1,\"o\")\n",
        "  if L1_ur is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ur,label=\"random uncertainty\",capsize=5)\n",
        "  if L1_us is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_us,label=\"systematic uncertainty\",capsize=5)\n",
        "  if L1_ut is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ut,label=\"total uncertainty\",capsize=5)\n",
        "  ax1.legend()\n",
        "  ax1.set_xlabel(\"wavelength (nm)\")\n",
        "  ax1.set_ylabel(\"radiance\")\n",
        "  ax1.set_title(\"L1 uncertainties\")\n",
        "  if L1_cov is not None:\n",
        "    ax2.set_title(\"L1 correlation\")\n",
        "    cov_plot=ax2.imshow(L1_corr)\n",
        "    plt.colorbar(cov_plot,ax=ax2)\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and make the plots for the L1 data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr) # make and display plot"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Step 4* - Propagate uncertainties with an error correlation matrix \n",
        "\n",
        "Instead of separately propagating the random and systematic uncertainties, we can also achieve the same result by first combining the random and systematic uncertainties on the input, and then propagating the total uncertainties and their error correlaiton. In this case, the error correlation needs to be explicitely passed to the `propagate_standard' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# first combine the random and systemtic uncertainties on the inputs\n",
        "L0_ut=(L0_ur**2+L0_us**2)**0.5\n",
        "gains_tot=(gains_ur**2+gains_us**2)**0.5\n",
        "\n",
        "#combine the error correlation matrices on the inputs (by combining the error covariances)\n",
        "L0_cov=punpy.convert_corr_to_cov(np.eye(len(L0_ur)),L0_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L0_us),len(L0_us))),L0_us)\n",
        "L0_corr=punpy.correlation_from_covariance(L0_cov)\n",
        "\n",
        "gains_cov=punpy.convert_corr_to_cov(np.eye(len(gains_ur)),gains_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(gains_us),len(gains_us))),gains_us)\n",
        "gains_corr=punpy.correlation_from_covariance(gains_cov)\n",
        "\n",
        "#propagate the combined uncertainties and error correlation\n",
        "L1_ut, L1_corr=prop.propagate_standard(calibrate,[L0,gains],\n",
        "      [L0_ut,gains_tot],[L0_corr,gains_corr], return_corr=True)\n",
        "\n",
        "#print results\n",
        "print(\"L1:    \",L1)\n",
        "print(\"L1_ut: \",L1_ut)\n",
        "print(\"L1_corr:\\n\",L1_corr)\n",
        "make_plots_L1(L1,L1_ut=L1_ut,L1_corr=L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Exercise**\n",
        "\n",
        "As an exercise, let's add an additional variable to the measurement function, and propagate uncertainties. \n",
        "Starting from the above example of calibrating an in-situ instrument by applying gains to the digital numbers, \n",
        "let's now add some dark measurements, which are subtracted from the digital numbers.\n",
        "\n",
        "We here provide the updated measurement function and additional input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# updated measurement function\n",
        "def calibrate(L0,gains,dark):\n",
        "   return (L0-dark)*gains\n",
        "\n",
        "# additional input quantity\n",
        "dark = np.array([0.05,0.03,0.04,0.05,0.06])\n",
        "dark_ur = np.array([0.02,0.02,0.02,0.02,0.02])  # random uncertainty"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now try yourself to propagate the uncertainties through this measurement function, by adapting the examples above. (Note that there are here no systematic uncertainties on the darks, so set these to zero if you need them)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enter your code here"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Step 5* - Error correlation between variables\n",
        "\n",
        "In addition to having a correlation along one or more dimensions of a given variable, it is also possible two variables are correlated (for example because they are measured using the same sensor). This can be specified in punpy by using the corr_between keyword. In the example below, the systematic errors in the darks and L0 data are fully correlated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We here define some systematic uncertainties for the darks, which are the same as for the digital numbers\n",
        "dark_us = L0_us\n",
        "\n",
        "# We then define how the errors for the different variables are correlated\n",
        "corr_var=np.array([[1,0,1],   # here a 1 means the variables are fully correlated, and a 0 means uncorrelated\n",
        "                   [0,1,0],   # on the diagonal there are 1's because each variable is fully correlated with itself\n",
        "                   [1,0,1]])  # there are also 1's on the (0,2) and (2,0) locations, indicating the 1st and last variable (i.e. L0 and dark) are correlated \n",
        "\n",
        "# We then recalculate the uncertainties and make a plot\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,L0_us],corr_between=corr_var)\n",
        "\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "\n",
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## *Step 6* - Punpy keywords"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many keywords that can be passed to the punpy functions to control the detailed behaviour. For a full description we refer to the [punpy documentation](https://punpy.readthedocs.io/en/latest/content/generated/punpy.mc.mc_propagation.MCPropagation.propagate_standard.html). Two features we would like to highlight is the ability to return the MC samples that were used, for manual inspection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L1_ut, L1_corr, MCsamples_L1, MCsamples_L0=prop.propagate_standard(calibrate,[L0,gains,dark],\n",
        "      [L0_ut,gains_tot,dark_ur],[L0_corr,gains_corr,\"rand\"], return_corr=True, return_samples=True)  #the return_samples keyword is set to True\n",
        "print(MCsamples_L0,MCsamples_L1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In these samples, we can see that there are some negative values in the dark samples. Depending on the use case, this might be considered unphysical. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to use different probability density functions (PDF) instead of the default Gaussian PDF. E.g. it is possible to set a lower boundary on the values in the MCsamples of the inputs, and thus avoid these negative valued:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L1_ut, MCsamples_L1, MCsamples_L0 = prop.propagate_standard(calibrate,[L0,gains,dark],\n",
        "      [L0_ut,gains_tot,dark_ur],[L0_corr,gains_corr,\"rand\"], return_corr=False, return_samples=True, pdf_shape=\"truncated_gaussian\", pdf_params={\"min\":0.})  #the pdf shape is set to truncated gaussian, and pdf_param is a dictionary that allows to set the minimum and maximum value\n",
        "print(MCsamples_L0,MCsamples_L1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are now no negative values anymore. (Note that this does reduce the uncertainties somewhat, so use with caution!)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **link to Next Exercise**\n",
        "We have now finished running through the first exercise going over the basic functionalities of punpy. \n",
        "\n",
        "In [Exercise 2](https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/LPS_training_exercise2.ipynb), the [**obsarray**](https://obsarray.readthedocs.io/en/latest/) and [**punpy**](https://punpy.readthedocs.io/en/latest/) functionalities for dealing with multi-dimension datasets will be showcased. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM40nBwQDY7evd+aKCRtcm/",
      "include_colab_link": true,
      "name": "LPS_training_exercise1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
