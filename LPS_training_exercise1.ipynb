{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/comet-toolkit/comet_training/blob/main/vhroda_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q--XLoo4Z325"
      },
      "source": [
        "**LPS hands-on training session: CoMet Toolkit: Uncertainties made easy**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4pxZ7fqqZ6jS"
      },
      "source": [
        "The aim of this training is to get you familiar with the CoMet toolkit, and uncertainties in general. We have already run through some theoretical background. Next, we will use a set of jupyter notebook, hosted on google colab, to run through three exercises on the use of the CoMet toolkit.\n",
        "- In the first exercise, we will run together through some of the basic functionality of the punpy tool.\n",
        "- In the second exercise, you will independently run through the jupyter notebook we have prepared. This will introduce some of the most important features of the obsarray and punpy tools. \n",
        "- In the third exercise, you will either try to apply these tools to propagate uncertainties for your own usecase, or alternatively try to implement uncertainty propagation for a usecase we have provided. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MzrTVRTqaNE3"
      },
      "source": [
        "We first install the obsarray package (flag handling and accessing uncertainties), the punpy package (uncertainty propagation) and the matheo package (for band integration)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install obsarray>=1.0.1\n",
        "!pip install punpy>=1.0.4\n",
        "!pip install matheo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also import all the relevant python packages we use in this training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xarray as xr\n",
        "import numpy as np\n",
        "import punpy\n",
        "from matheo.band_integration import band_integration\n",
        "from obsarray.templater.dataset_util import DatasetUtil\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If this import fails, it is likely because the pip installation has not properly updated in the Google colab session. Please restart session (in runtime tab above)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we clone the comet_training repository, which contains all the datasets used in this training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!git clone https://github.com/comet-toolkit/comet_training.git"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise 1: simple sensor calibration example**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this exercise, the aim is to get familiar with the basic functionality of punpy. Here, punpy will be used as a standalone tool (i.e. without combining it with obsarray functionality). We will use an example of a very basic sensor calibration, where we have some digital numbers for the signal (referred to as L0), digital numbers for the dark measurement, and the gains (typically obtained from a lab calibration) to convert these to a physical quantity (referred to as L1). This could e.g. be for a radiance measurement by an insitu instrument.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we define our measurement function. For use in punpy, this measurement function needs to be written as a Python function that takes the input quantities (on which we have uncertainties available) as arhuments and the measurand (to which we want to propagate the uncertainties) as return. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your measurement function\n",
        "def calibrate(L0,gains,dark):\n",
        "   return (L0-dark)*gains"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, the measurement function is a very simple analytical function. However in practise, this measurement function can contain as much complexity (including calls to other packages/external software, ...) as needed. The measurement function is to some extend treated as a black box, as long as the input quantities and measurand are structured as expected."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define some example input data. For your own usecase, you would need to have this information available from other sources (i.e. the uncertainties on your inputs needs to be understood prior to using punpy). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your data\n",
        "wavs = np.array([350,450,550,650,750])\n",
        "L0 = np.array([0.43,0.8,0.7,0.65,0.9])\n",
        "dark = np.array([0.05,0.03,0.04,0.05,0.06])\n",
        "gains = np.array([23,26,28,29,31])\n",
        "\n",
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "L0_us = np.ones(5)*0.03  # systematic uncertainty of 0.03\n",
        "                         # (common between bands)\n",
        "gains_ur = np.array([0.5,0.7,0.6,0.4,0.1])  # random uncertainty\n",
        "gains_us = np.array([0.1,0.2,0.1,0.4,0.3])  # systematic uncertainty\n",
        "# (different for each band but fully correlated)\n",
        "dark_ur = np.array([0.01,0.002,0.006,0.002,0.015])  # random uncertainty"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After defining the data, the resulting uncertainty budget can then be calculated with punpy using the MC methods as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,np.zeros(5)])\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "print(\"L1:    \",L1)\n",
        "print(\"L1_ur: \",L1_ur)\n",
        "print(\"L1_us: \",L1_us)\n",
        "print(\"L1_ut: \",L1_ut)\n",
        "print(\"L1_cov:\\n\",L1_cov)\n",
        "print(\"L1_corr:\\n\",L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then define some plots to inspect the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_plots_L1(L1,L1_ur=None,L1_us=None,L1_ut=None,L1_corr=None):\n",
        "  if L1_cov is not None:\n",
        "    fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n",
        "  else:\n",
        "    fig,ax1 = plt.subplots(1,figsize=(5,5))\n",
        "\n",
        "  ax1.plot(wavs,L1,\"o\")\n",
        "  if L1_ur is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ur,label=\"random uncertainty\",capsize=5)\n",
        "  if L1_us is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_us,label=\"systematic uncertainty\",capsize=5)\n",
        "  if L1_ut is not None:\n",
        "    ax1.errorbar(wavs,L1,yerr=L1_ut,label=\"total uncertainty\",capsize=5)\n",
        "  ax1.legend()\n",
        "  ax1.set_xlabel(\"wavelength (nm)\")\n",
        "  ax1.set_ylabel(\"radiance\")\n",
        "  ax1.set_title(\"L1 uncertainties\")\n",
        "  if L1_cov is not None:\n",
        "    ax2.set_title(\"L1 correlation\")\n",
        "    cov_plot=ax2.imshow(L1_corr)\n",
        "    plt.colorbar(cov_plot,ax=ax2)\n",
        "  plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and make the plots for the L1 data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of separately propagating the random and systematic uncertainties, we can also achieve the same result by first combining the random and systematic uncertainties on the input, and then propagating the total uncertainties. In this case, the error correlation needs to be explicitely passed to the `propagate_standard' function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L0_tot=(L0_ur**2+L0_us**2)**0.5\n",
        "L0_cov=punpy.convert_corr_to_cov(np.eye(len(L0_ur)),L0_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L0_us),len(L0_us))),L0_us)\n",
        "L0_corr=punpy.correlation_from_covariance(L0_cov)\n",
        "gains_tot=(gains_ur**2+gains_us**2)**0.5\n",
        "gains_cov=punpy.convert_corr_to_cov(np.eye(len(gains_ur)),gains_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(gains_us),len(gains_us))),gains_us)\n",
        "gains_corr=punpy.correlation_from_covariance(gains_cov)\n",
        "L1_ut, L1_corr=prop.propagate_standard(calibrate,[L0,gains,dark],\n",
        "      [L0_ut,gains_tot,dark_ur],[L0_corr,gains_corr,\"rand\"], return_corr=True)\n",
        "\n",
        "print(\"L1:    \",L1)\n",
        "print(\"L1_ut: \",L1_ut)\n",
        "print(\"L1_corr:\\n\",L1_corr)\n",
        "make_plots_L1(L1,L1_ut=L1_ut,L1_corr=L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to propagating random (uncorrelated) and systematic (fully correlated) uncertainties it is also possible to propagate uncertainties associated with structured errors. If we know the covariance matrix for each of the input quantities, it is straigtforward to propagate these. In the below example we assume the L0 data and dark data to be uncorrelated (their covariance matrix is a, diagonal matrix) and gains to be a custom covariance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# your uncertainties\n",
        "L0_ur = L0*0.05  # 5% random uncertainty\n",
        "dark_ur = np.array([0.01,0.002,0.006,0.002,0.015])  # random uncertainty\n",
        "\n",
        "L0_cov=punpy.convert_corr_to_cov(np.eye(len(L0_ur)),L0_ur)\n",
        "dark_cov=punpy.convert_corr_to_cov(np.eye(len(dark_ur)),dark_ur )\n",
        "gains_cov= np.array([[0.45,0.35,0.30,0.20,0.05],\n",
        "                    [0.35,0.57,0.32,0.30,0.07],\n",
        "                    [0.30,0.32,0.56,0.24,0.06],\n",
        "                    [0.20,0.30,0.24,0.44,0.04],\n",
        "                    [0.05,0.07,0.06,0.04,0.21]])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "L1_ut,L1_corr=prop.propagate_cov(calibrate,[L0,gains,dark],\n",
        "                                 [L0_cov,gains_cov,dark_cov],return_corr=True)\n",
        "\n",
        "make_plots_L1(L1,L1_ut=L1_ut,L1_corr=L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In addition to having a correlation along one or more dimensions of a given variable, it is also possible two variables are correlated. This can be specified in punpy by using the corr_between keyword. In the example below, the systematic errors in the darks and L0 data are fully correlated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prop=punpy.MCPropagation(10000)\n",
        "L1=calibrate(L0,gains,dark)\n",
        "\n",
        "corr_var=np.array([[1,0,1],[0,1,0],[1,0,1]])\n",
        "\n",
        "L1_ur=prop.propagate_random(calibrate,[L0,gains,dark],\n",
        "      [L0_ur,gains_ur,dark_ur])\n",
        "L1_us=prop.propagate_systematic(calibrate,[L0,gains,dark],\n",
        "      [L0_us,gains_us,L0_us],corr_between=corr_var)\n",
        "L1_ut=(L1_ur**2+L1_us**2)**0.5\n",
        "L1_cov=punpy.convert_corr_to_cov(np.eye(len(L1_ur)),L1_ur)+\\\n",
        "       punpy.convert_corr_to_cov(np.ones((len(L1_us),len(L1_us))),L1_us)\n",
        "L1_corr=punpy.correlation_from_covariance(L1_cov)\n",
        "make_plots_L1(L1,L1_ur,L1_us,L1_ut,L1_corr)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are many keywords that can be passed to the punpy functions to control the detailed behaviour. For a full description we refer to the punpy documentation (see e.g. https://punpy.readthedocs.io/en/latest/content/generated/punpy.mc.mc_propagation.MCPropagation.propagate_standard.html). Two features we would like to highlight is the ability to return the MC samples that were used, for manual inspection:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L1_ut, L1_corr, MCsamples_L1, MCsamples_L0=prop.propagate_standard(calibrate,[L0,gains,dark],\n",
        "      [L0_ut,gains_tot,dark_ur],[L0_corr,gains_corr,\"rand\"], return_corr=True, return_samples=True)\n",
        "print(MCsamples_L0,MCsamples_L1)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is also possible to use different probability density functions (PDF) instead of the default Gaussian PDF. E.g. it is possible to set a lower boundary on the values in the MCsamples of the inputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "L1_ut=prop.propagate_standard(calibrate,[L0,gains,dark],\n",
        "      [L0_ut,gains_tot,dark_ur],[L0_corr,gains_corr,\"rand\"], return_corr=False, pdf_shape=\"truncated_gaussian\", pdf_params={\"min\":0.})\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have now finished running through the first exercise going over the basic functionalities of punpy. Next, please run through the second exercise yourself (https://github.com/comet-toolkit/comet_training/blob/main/LPS_training_exercise2.ipynb), where some of the most important features of the obsarray and punpy tools will be introduced. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyM40nBwQDY7evd+aKCRtcm/",
      "include_colab_link": true,
      "name": "vhroda_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
